{
  "hash": "78462940012390e16c3f0abbac8cbff1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Web Scraping\"\nauthor: \"Ee Fook Ming\"\ndate: \"15 March 2025\" \ndate-modified: \"last-modified\"\nexecute:\n  eval: true\n  echo: true\n  message: false\n  freeze: true\n  warning: false\n---\n\n\n\n# Climate Historical Daily Records\n\nAs the climate historical daily records can only be retrieved from [Meteorological Service Singapore](https://www.weather.gov.sg/climate-historical-daily/) for a specific month of one station at a time, we have written the below code chunk to retrieve all climate daily records from 2015-2024 and compiled into one single csv file.\n\n# Set up R environment\n\nFor the purpose of data scraping and compiling, the following packages are installed and loaded into R environment using `p_load()` of **pacman** package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\npacman::p_load(httr, readxl, dplyr, readr)\n```\n:::\n\n\n\n# Download Data\n\nFirst we read in the S`tation_Records.xlsx` using below code chunk to retrieve station codes and their corresponding types. The station codes form part of a text string used to construct hyperlinks, which retrieve data directly from the weather website.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstation_records <- read_excel(\"data/Station_Records.xlsx\")\n\n# Obtain a list of station codes\nstation_codes <- station_records$`code`\nstation_type <- station_records$`Station_Type`\n\nprint(station_codes)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n [1] \"S104\" \"S105\" \"S109\" \"S86\"  \"S63\"  \"S120\" \"S55\"  \"S64\"  \"S90\"  \"S92\" \n[11] \"S61\"  \"S24\"  \"S114\" \"S121\" \"S11\"  \"S50\"  \"S118\" \"S107\" \"S39\"  \"S101\"\n[21] \"S44\"  \"S117\" \"S33\"  \"S31\"  \"S71\"  \"S122\" \"S66\"  \"S112\" \"S08\"  \"S07\" \n[31] \"S40\"  \"S108\" \"S113\" \"S111\" \"S119\" \"S116\" \"S29\"  \"S94\"  \"S06\"  \"S106\"\n[41] \"S81\"  \"S77\"  \"S25\"  \"S102\" \"S80\"  \"S60\"  \"S36\"  \"S110\" \"S84\"  \"S79\" \n[51] \"S43\"  \"S78\"  \"S72\"  \"S23\"  \"S88\"  \"S89\"  \"S115\" \"S82\"  \"S35\"  \"S69\" \n[61] \"S46\"  \"S123\" \"S91\" \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(unique(station_type))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Full AWS Station\" \"Closed Station\"   \"Rainfall Station\"\n```\n\n\n:::\n:::\n\n\n\nThe output shows there are three station types:\n\n-   Full AWS station: collects weather data on rainfall, temperature, and wind.\n\n-   Rainfall station: collects rainfall data only.\n\n-   Closed station: a station that is no longer operational.\n\nThese station identifiers are useful for data filtering, analysis, and visualization based on specific station characteristics.\n\nThe below code chunk systematically downloads daily climate data from Singaporeâ€™s weather service for all weather stations, covering years 2015 through 2024. It constructs URLs dynamically based on station codes, year, and month, retrieves CSV files, filters out incomplete or invalid entries, and then consolidates valid data into a single CSV file (`Climate_Data_2015_2024_v4.csv`). The primary intention is to compile a structured, comprehensive dataset of climate measurements for subsequent data processing, analysis or visualization tasks.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(httr)\nlibrary(readr)\n\n# Set base URL template and save file path\nbase_url_template <- \"http://www.weather.gov.sg/files/dailydata/DAILYDATA_%s_%d%s.csv\"\nsave_file <- \"data/climate_historical_daily_records/Climate_Data_2015_2024_v4.csv\"\ndir.create(dirname(save_file), recursive = TRUE, showWarnings = FALSE)\n\n# Define expected column names (preserve exact spacing)\ncolumn_names <- c(\n  \"Station\", \"Year\", \"Month\", \"Day\", \n  \"Daily Rainfall Total (mm)\", \"Highest 30 min Rainfall (mm)\", \n  \"Highest 60 min Rainfall (mm)\", \"Highest 120 min Rainfall (mm)\", \n  \"Mean Temperature (C)\", \"Maximum Temperature (C)\", \n  \"Minimum Temperature (C)\", \"Mean Wind Speed (km/h)\", \n  \"Max Wind Speed (km/h)\"\n)\n\n# Ensure CSV file has proper headers only if it does not exist\nif (!file.exists(save_file)) {\n  write.table(data.frame(matrix(ncol = length(column_names), nrow = 0, dimnames = list(NULL, column_names))),\n              save_file, sep = \",\", row.names = FALSE, col.names = TRUE, quote = FALSE)\n}\n\n# Function to download and append data\ndownload_and_append <- function(station_code, year, month) {\n  month_str <- sprintf(\"%02d\", month)  # Format month as \"01\", \"02\", etc.\n  file_url <- sprintf(base_url_template, station_code, year, month_str)\n\n  response <- tryCatch({\n    GET(file_url)\n  }, error = function(e) {\n    message(sprintf(\"Error fetching: %s\", file_url))\n    return(NULL)\n  })\n\n  if (!is.null(response) && status_code(response) == 200) {\n    # Read CSV content without modifying column names\n    csv_data <- tryCatch({\n      read_csv(content(response, \"raw\"), show_col_types = FALSE, col_names = FALSE, skip = 1, check.names = FALSE)\n    }, error = function(e) {\n      message(sprintf(\"Failed to read CSV: %s\", file_url))\n      return(NULL)\n    })\n\n    # Ensure correct column alignment and remove empty rows\n    if (!is.null(csv_data) && ncol(csv_data) == length(column_names)) {\n      colnames(csv_data) <- column_names  # Assign exact column names\n\n      csv_data <- csv_data[rowSums(is.na(csv_data) | csv_data == \"\") < ncol(csv_data), ]  # Remove empty rows\n      \n      if (nrow(csv_data) > 0) {\n        write.table(csv_data, save_file, sep = \",\", row.names = FALSE, col.names = FALSE, append = TRUE, quote = FALSE)\n        message(sprintf(\"Appended data for %s - %d-%s\", station_code, year, month_str))\n      } else {\n        message(sprintf(\"Skipping empty dataset: %s\", file_url))\n      }\n    } else {\n      message(sprintf(\"Data format mismatch: %s\", file_url))\n    }\n  } else {\n    message(sprintf(\"Failed to download: %s\", file_url))\n  }\n}\n\n# Loop through each station code and fetch data\nfor (station_code in station_codes) {\n  for (year in 2015:2024) {\n    for (month in 1:12) {\n      download_and_append(station_code, year, month)\n    }\n  }\n}\n\nprint(\"Download process completed for all stations.\")\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}