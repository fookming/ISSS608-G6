---
title: "Data Preparation"
author: "Ee Fook Ming, Nguyen Bao Thu Phuong & Shreya Agarwal"
date: "18 March 2025" 
date-modified: "last-modified"
execute:
  eval: true
  echo: true
  message: false
  freeze: true
  warning: false
---

# Overview

This section outlines the data preparation process, including the exclusion of stations with extensive missing data and the imputation of missing values for selected stations, ensuring data quality for subsequent visual analytics in our project.

## The Data

-   Scraped weather data from Meteorological Service Singapore stored in `Climate_Data_2015_2024.csv` .
-   Weather Station records from [weather.gov.sg](https://www.weather.gov.sg/wp-content/uploads/2024/08/Station_Records.pdf), stored in `Station_Records.csv` .

# Install R packages

The below code chunk uses `p_load()` to load and install the required packages into R environment.

```{r}
pacman::p_load(tidyverse, readr, openxlsx, zoo)
```

# Data Import and Preparation

## Daily Weather Records

First we read in the daily weather records scraped from [weather.gov.sg](https://www.weather.gov.sg/climate-historical-daily/) into tibble dataframe `climate_raw` using `read_csv()` .

```{r}
climate_raw = read_csv("data/climate_historical_daily_records/Climate_Data_2015_2024.csv")
```

Next we check if the scraped data contains any duplicate.

```{r}
duplicate <- climate_raw %>% 
  group_by_all() %>% 
  filter(n()>1) %>% 
  ungroup()
  
duplicate
```

The output shows there is no duplicates.

The below code chunk uses `summary()` to have an overview of the scraped dataset.

```{r}
summary(climate_raw)
```

The output shows there are many null values, while the min max range of all weather measurements are in the normal range.

## Weather Station Records

Next we read in the stations details to retrieve station types and coordinates. For our Shiny application, we focus solely on **Full AWS** stations, as they provide comprehensive data across all weather parameters. Other station types, such as Closed Stations and Daily Rainfall Stations (which only record rainfall metrics), are excluded from further analysis.

```{r}
# Read station metadata and filter for Full AWS stations
station = read_csv("data/Station_Records.csv") |> 
  filter(Station_Type == "Full AWS Station")
```

The below code chunk checks for duplicate.

```{r}
duplicate <- station %>%    
  group_by_all() %>%    
  filter(n()>1) %>%    
  ungroup()

duplicate
```

The output shows there is no duplicate.

# Data Wrangling

## Join Station with Climate dataframe

First we join `station` with `climate_raw` dataframe using `inner_join()` to include only records from active Full AWS Station.

```{r}
# Join to retain only Full AWS station records
climate = inner_join(station,climate_raw, by = "Station")
```

The below code chunk checks for duplicates in the joined dataframe.

```{r}
duplicate <- climate %>%    
  group_by_all() %>%    
  filter(n()>1) %>%    
  ungroup()    

duplicate
```

The output shows there is no duplicate in `climate` dataframe.

Next we filter climate dataset for only relevant columns for our weather analysis.

```{r}
# Select relevant columns
climate <- climate |> 
  select(Station, Station_Type, Year, Month, Day,
         `Daily Rainfall Total (mm)`,
         `Mean Temperature (Celsius)`,
         `Maximum Temperature (Celsius)`,
         `Minimum Temperature (Celsius)`,
         `Mean Wind Speed (km/h)`)
```

The final `climate` dataframe consists of **10 columns**, including the **5 key weather parameters** selected for further analysis.

## Missing Rainfall records

Next we check for null records from the key parameter `Daily Rainfall Total (mm)`.

```{r}
# Count missing values for each Station, Year, and Month 
missing_rain_counts <- climate %>%   
  group_by(Station, Year, Month) %>%   
  summarise(
    `Missing Daily Rainfall Total` = sum(is.na(`Daily Rainfall Total (mm)`))   ) %>%  
    # Exclude rows where all missing counts are zero
  filter(`Missing Daily Rainfall Total` > 0) |>
  ungroup()
```

```{r}
# Count the number of months per station where missing days exceed 15 
missing_rain_summary <- missing_rain_counts %>%   
  filter(`Missing Daily Rainfall Total` >= 15) %>%   
  group_by(Station, Year) %>%   
  summarise(Count_Months = n()) %>%   
  arrange(Year)|>   
  pivot_wider(names_from = Year, values_from = Count_Months) |>   
  ungroup()
```

The output reveals 7 stations with ≥15 days of missing Daily Rainfall Total (mm) in more than one month over the years. Semakau Island station has a full year of missing data in 2019, while 2015–2017 period shows a high prevalence of missing data across stations.

## Missing Temperature Records

We carry out the same check for `Mean Temperature (Celsius)`.

```{r}
# Count missing values for each Station, Year, and Month 
missing_temp_counts <- climate %>%   
  group_by(Station, Year, Month) %>%   
  summarise(     
    `Missing Mean Temperature` = sum(is.na(`Mean Temperature (Celsius)`))) %>%   
  # Exclude rows where all missing counts are zero
  filter(`Missing Mean Temperature` > 0) |>
  ungroup()
```

```{r}
# Count the number of months per station where missing days exceed 15 
missing_temp_summary <- missing_temp_counts %>%    
  filter(`Missing Mean Temperature` >= 15) %>%   
  group_by(Station, Year) %>%   
  summarise(Count_Months = n()) %>%   
  arrange(Year)|>   
  pivot_wider(names_from = Year, values_from = Count_Months) |>   
  ungroup()
```

The same 7 stations with significant missing `Daily Rainfall Total (mm)` are also among the 15 stations with missing `Mean Temperature (Celsius)` data, where more than 15 days are missing for certain months. Some stations have full years of missing data, with 2015–2017 particularly shows a high prevalence of missing data across stations.

## Exclude stations with many missing records

Given the extensive missing data from 2015–2017, we will retain data from 2018 onwards. For each station, we will count the total number of months with more than 15 days missing data across the years and flag stations with more than 3 missing months. Those stations will be excluded from the main `climate` dataset.

```{r}
filter_invalid_stations <- function(summary_df, from_year = 2018) {
  # Get only year columns ≥ from_year
  year_cols <- summary_df |> 
    select(where(is.numeric)) |> 
    select(matches("^[0-9]{4}$")) |> 
    select(as.character(from_year):last_col()) |> 
    colnames()

  summary_df |> 
    rowwise() |> 
    mutate(total_flagged_months = sum(c_across(all_of(year_cols)), na.rm = TRUE)) |> 
    filter(total_flagged_months >= 3) |> 
    pull(Station)
}

```

```{r}
# Get invalid stations (stations to exclude)
invalid_rain_stations <- filter_invalid_stations(missing_rain_summary)
invalid_temp_stations <- filter_invalid_stations(missing_temp_summary)
```

```{r}
# Combine all stations to exclude
stations_to_exclude <- union(invalid_rain_stations, invalid_temp_stations) 
```

The above 5 stations will be excluded. The below code chunk applies this filter on the `climate` dataframe.

```{r}
# Now filter the full dataset to only include stations from 2018-2024
climate_final <- climate |> 
  filter(!(Station %in% stations_to_exclude), Year >= 2018, Year <= 2024)

summary(climate_final)
```

`climate_final` is the filtered dataset obtained. Below is the list of the final 15 AWS stations that we will be focusing our analysis on.

```{r}
climate_final %>% 
  distinct(Station) %>% 
  arrange(Station) %>% 
  mutate(Station_ID = row_number())
```

## Replacing Missing Values using SMA 5 days

The below code chunk counts the number of missing values in each column of `climate_final`.

```{r}
climate_final %>% 
  summarise(across(where(is.numeric), ~sum(is.na(.)), .names = "missing_{.col}"))
```

The output reveals multiple missing values across the five weather parameters. To address this, we will first apply a 5-day backward moving average for imputation. However, for cases where this method cannot be applied (e.g., missing values at the beginning of the time series), we will use a 5-day forward moving average as a fallback.

```{r}
#| eval: false
# === Load dataset ===
climate <- climate_final
original <- climate

# === Columns to impute ===
impute_cols <- c("Daily Rainfall Total (mm)",
                 "Mean Temperature (Celsius)",
                 "Maximum Temperature (Celsius)",
                 "Minimum Temperature (Celsius)",
                 "Mean Wind Speed (km/h)")

# === Backward SMA-5 for first 4 rows ===
# === Backward SMA-5 (recursive fill for rows 4 to 1) ===
for (i in 4:1) {
  window <- climate[(i + 1):(i + 5), impute_cols]
  sma_values <- colMeans(window, na.rm = TRUE) %>% round(1)
  climate[i, impute_cols] <- as.list(sma_values)
}


# === Forward SMA-5 for all other NAs ===
for (col in impute_cols) {
  na_indices <- which(is.na(climate[[col]]))
  for (idx in na_indices) {
    if (idx >= 5) {
      sma_val <- mean(climate[[col]][(idx - 5):(idx - 1)], na.rm = TRUE) %>% round(1)
      if (!is.nan(sma_val)) {
        climate[[col]][idx] <- sma_val
      }
    }
  }
}

# === Save updated CSV ===
write_csv(climate, "data/climate_historical_daily_records/climate_final_2018_2024.csv")
```

The final prepared dataset for our analysis is saved as `climate_final_2018_2024.csv` with no missing data.

The below code chunk reads the final dataset from `climate_final_2018_2024.csv`for further analysis.

```{r}
climate = read_csv('data/climate_historical_daily_records/climate_final_2018_2024.csv')
```

We verify the final `climate` dataframe for missing data using below code chunk. The output confirms that all missing values have been successfully imputed.

```{r}
climate %>% 
  summarise(across(where(is.numeric), ~sum(is.na(.)), .names = "missing_{.col}"))
```

The below code chunk provides a summary of the imputed `climate` dataframe.

```{r}
summary(climate)
```
